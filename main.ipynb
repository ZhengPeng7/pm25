{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from config import Config\n",
    "from network import Network\n",
    "\n",
    "\n",
    "# Configurations\n",
    "testset_num = 5  # int(sys.argv[1]) if len(sys.argv) == 2 else 1\n",
    "config = Config(testset_num=testset_num)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.GPUs\n",
    "\n",
    "if not os.path.exists(config.save_dir):\n",
    "    os.makedirs(config.save_dir)\n",
    "print('save_dir:', config.save_dir)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "\n",
    "# Data generator\n",
    "from data import gen_paths_of_image, DataGen\n",
    "image_paths = gen_paths_of_image(root_path='../datasets/PM2.5data/fog_1508_data')\n",
    "TBVs = np.loadtxt('data_preparation/TBVs.txt').tolist()\n",
    "entropies = np.loadtxt('data_preparation/entropies.txt').tolist()\n",
    "pm = np.loadtxt('data_preparation/pm25.txt').tolist()\n",
    "\n",
    "num_files = [0, 150, 90, 120, 151, 150, 120]\n",
    "\n",
    "image_paths_test = image_paths[np.sum(num_files[:config.testset_num]):np.sum(num_files[:config.testset_num+1])]\n",
    "TBVs_test = TBVs[np.sum(num_files[:config.testset_num]):np.sum(num_files[:config.testset_num+1])]\n",
    "entropies_test = entropies[np.sum(num_files[:config.testset_num]):np.sum(num_files[:config.testset_num+1])]\n",
    "pm_test = pm[np.sum(num_files[:config.testset_num]):np.sum(num_files[:config.testset_num+1])]\n",
    "\n",
    "image_paths = image_paths[:np.sum(num_files[:config.testset_num])] + image_paths[np.sum(num_files[:config.testset_num+1]):]\n",
    "TBVs = TBVs[:np.sum(num_files[:config.testset_num])] + TBVs[np.sum(num_files[:config.testset_num+1]):]\n",
    "entropies = entropies[:np.sum(num_files[:config.testset_num])] + entropies[np.sum(num_files[:config.testset_num+1]):]\n",
    "pm = pm[:np.sum(num_files[:config.testset_num])] + pm[np.sum(num_files[:config.testset_num+1]):]\n",
    "\n",
    "method_on_TBV = 0\n",
    "if method_on_TBV == 0:\n",
    "    # Method-1 on TBVs\n",
    "    TBV_min = np.min(TBVs)\n",
    "    TBV_range = np.max(TBVs) - TBV_min\n",
    "    entro_min = np.min(entropies)\n",
    "    entro_range = np.max(entropies) - entro_min\n",
    "    TBVs = (TBVs - TBV_min) / TBV_range\n",
    "    entropies = (entropies - entro_min) / entro_range\n",
    "    TBVs_test = (TBVs_test - TBV_min) / TBV_range\n",
    "    entropies_test = (entropies_test - entro_min) / entro_range\n",
    "elif method_on_TBV == 1:\n",
    "    # Method-2 on TBVs\n",
    "    TBVs = np.log(TBVs) * 1.\n",
    "    TBVs_test = np.log(TBVs_test) * 1.\n",
    "\n",
    "seed = 7\n",
    "random.seed(seed); random.shuffle(image_paths)\n",
    "random.seed(seed); random.shuffle(TBVs)\n",
    "random.seed(seed); random.shuffle(entropies)\n",
    "random.seed(seed); random.shuffle(pm)\n",
    "\n",
    "gen_train = DataGen(image_paths, TBVs, entropies, pm, batch_size=config.batch_size)\n",
    "gen_test = DataGen(image_paths_test, TBVs_test, entropies_test, pm_test, batch_size=config.batch_size_test)\n",
    "\n",
    "print('Train Len', gen_train.data_len)\n",
    "print('Test Len', gen_test.data_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "best_preds, best_MAPEs, best_MAPE_mean, best_path_weights = [1e6], [1e6], 1e6, 'zhengpeng'\n",
    "model = Network(pretrain=True)\n",
    "model = nn.DataParallel(model)\n",
    "model = model.cuda()\n",
    "criterion = torch.nn.MSELoss(reduction='sum').to(config.device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.lr, betas=(0.9, 0.999), weight_decay=config.weight_decay)\n",
    "for epoch in range(config.epochs):\n",
    "    gen_train = DataGen(image_paths, TBVs, entropies, pm, batch_size=config.batch_size, training=True)\n",
    "    gen_test = DataGen(image_paths_test, TBVs_test, entropies_test, pm_test, batch_size=config.batch_size_test)\n",
    "    losses_curr = []\n",
    "    for idx_load in range(0, gen_train.data_len, gen_train.batch_size):\n",
    "        model.train()\n",
    "        batch_image, batch_TBV, batch_entropy, batch_pm = gen_train.gen_batch()\n",
    "        pm_pred = model(\n",
    "            torch.from_numpy(batch_image).float().cuda(),\n",
    "            torch.from_numpy(batch_TBV).float().cuda(),\n",
    "            torch.from_numpy(batch_entropy).float().cuda()\n",
    "        )\n",
    "        batch_TBV = np.squeeze(batch_TBV)\n",
    "        batch_entropy = np.squeeze(batch_entropy)\n",
    "        # with open('../preds.txt', 'a+') as fout:\n",
    "        #     fout.write('{:.3f}, {:.3f}, {:.3f}, {:.3f}\\n'.format(pm_pred[0].item(), batch_pm[0], batch_TBV[0], batch_entropy[0]))\n",
    "        #     fout.write('{:.3f}, {:.3f}, {:.3f}, {:.3f}\\n'.format(pm_pred[1].item(), batch_pm[1], batch_TBV[1], batch_entropy[1]))\n",
    "        loss = criterion(pm_pred, torch.tensor(batch_pm).float().cuda().unsqueeze(-1))\n",
    "        loss = loss.to(config.device)\n",
    "        losses_curr.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Val\n",
    "        if idx_load > 0 and 0 == (idx_load // gen_train.batch_size) % (gen_train.data_len // gen_train.batch_size // config.num_val_per_epoch):\n",
    "            model.eval()\n",
    "            pm_preds = []\n",
    "            MAPEs = []\n",
    "            for idx_load_test in range(0, gen_test.data_len, gen_test.batch_size):\n",
    "                batch_image, batch_TBV, batch_entropy, batch_pm = gen_test.gen_batch()\n",
    "                with torch.no_grad():\n",
    "                    pm_pred = model(\n",
    "                    torch.from_numpy(batch_image).float().cuda(),\n",
    "                    torch.from_numpy(batch_TBV).float().cuda(),\n",
    "                    torch.from_numpy(batch_entropy).float().cuda()\n",
    "                ).squeeze().item()\n",
    "                pm_preds.append(pm_pred)\n",
    "                batch_pm = np.squeeze(batch_pm)\n",
    "                MAPE = np.abs(pm_pred - batch_pm) * 100 / batch_pm\n",
    "                MAPEs.append(MAPE)\n",
    "            if np.mean(MAPEs) < best_MAPE_mean:\n",
    "                best_preds = pm_preds\n",
    "                best_MAPEs = MAPEs\n",
    "                best_MAPE_mean = np.mean(best_MAPEs)\n",
    "                best_dict_ckpt = {'epoch': epoch + 1, 'state_dict': model.state_dict(), 'loss': loss}\n",
    "                best_path_ckpt = os.path.join(config.save_dir, 'PMNet_epoch{}_idxload{}_loss{:.1f}_MAPE{:3.f}.pth'.format(epoch+1, idx_load, loss, best_MAPE_mean))\n",
    "                torch.save(best_dict_ckpt, best_path_ckpt)\n",
    "            print('\\tMAPE_mean = {:.3f}, best_MAPE_mean = {:.3f}'.format(np.mean(MAPEs), best_MAPE_mean))\n",
    "\n",
    "\n",
    "    loss = np.mean(losses_curr)\n",
    "    config.losses.append(loss)\n",
    "    print('epoch={}, loss={}, time={}m'.format(epoch+1, loss, int((time.time()-config.time_st)/60)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss plot\n",
    "if not os.path.exists(config.save_dir_test):\n",
    "    os.makedirs(config.save_dir_test)\n",
    "print('save_dir:', config.save_dir_test)\n",
    "np.savetxt(os.path.join(config.save_dir_test, 'loss.txt'), config.losses)\n",
    "plt.plot(config.losses)\n",
    "plt.legend(['losses'])\n",
    "plt.title('Loss')\n",
    "plt.savefig(os.path.join(config.save_dir_test, 'loss_plot.png'))\n",
    "\n",
    "\n",
    "results = np.hstack([\n",
    "    np.array(pm_test).reshape(-1, 1),\n",
    "    np.array(best_preds).reshape(-1, 1),\n",
    "    np.array(best_MAPEs).reshape(-1, 1)\n",
    "])\n",
    "\n",
    "\n",
    "path_results = best_path_ckpt.replace('weights', 'results').replace('PMNet', 'results').replace('.pth', '.csv')\n",
    "pd.DataFrame(results).to_csv(path_results, index=False, header=['Label', 'Predictions', 'MAPE(%)'])\n",
    "\n",
    "plt.plot(pm_test, 'r')\n",
    "plt.plot(best_preds, 'g')\n",
    "plt.plot(best_MAPEs, 'b')\n",
    "plt.legend(['Label', 'Prediction', 'MAPE'])\n",
    "plt.title('RM2.5')\n",
    "plt.savefig(os.path.join(config.save_dir_test, 'results_plot.png'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
